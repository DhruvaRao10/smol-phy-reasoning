{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a7c489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/23.3 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m18 packages\u001b[0m \u001b[2min 161ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 736ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 407ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 63ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.56.2\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m                                            \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 93ms\u001b[0m\u001b[0m                                               \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.24.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.22.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Cannot exactly port over the code from QWEN-3_4B_GRPO code from Unsloth \n",
    "# PEFT model is not required -> 0.6B model so quite small no need for LORA finetune will do a full finetune \n",
    "\n",
    "import os\n",
    "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\" # [NEW] Extra 30% context lengths!\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    # If you're not in Colab, just use pip install or uv pip install\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    pass # For Colab / Kaggle, we need extra instructions hidden below \\/\n",
    "\n",
    "\n",
    "#@title Colab Extra Install { display-mode: \"form\" }\n",
    "import os\n",
    "!pip install --upgrade -qqq uv\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    # If you're not in Colab, just use pip install!\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
    "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
    "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
    "    except: is_t4 = False\n",
    "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
    "    !uv pip install -qqq --upgrade \\\n",
    "        unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers\n",
    "    !uv pip install -qqq {get_triton}\n",
    "!uv pip install transformers==4.56.2\n",
    "!uv pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f98d7b73",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (ipython-input-1903864779.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1903864779.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    pip install --upgrade setuptools\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# upgrading setup tools due generationixin.utils import not working when loading the model and tokenizer for qwen3 \n",
    "pip install --upgrade setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2258555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# # load the tokenizer and the model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=\"auto\",\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# # prepare the model input\n",
    "# prompt = \"Give me a short introduction to large language model.\"\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": prompt}\n",
    "# ]\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize=False,\n",
    "#     add_generation_prompt=True,\n",
    "#     enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    "# )\n",
    "# model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# # conduct text completion\n",
    "# generated_ids = model.generate(\n",
    "#     **model_inputs,\n",
    "#     max_new_tokens=32768\n",
    "# )\n",
    "# output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# # parsing thinking content\n",
    "# try:\n",
    "#     # rindex finding 151668 (</think>)\n",
    "#     index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "# except ValueError:\n",
    "#     index = 0\n",
    "\n",
    "# thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "# content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "# print(\"thinking content:\", thinking_content)\n",
    "# print(\"content:\", content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd00fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2f648529f742d4972a3de5a26c01a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1f9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790dcd05cb6140d4993c39d798aa9c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab45359cdab445a0b8cc9dfb4aa2062a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32f38f5e2cf4b11986e199a3cbe8b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621d9df69c994258aba157235885766b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4664dc08be9045708a407219a2afc18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "/usr/local/lib/python3.12/dist-packages/torchao/quantization/quant_api.py:2525: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  * regex for parameter names, must start with `re:`, e.g. `re:language\\.layers\\..+\\.q_proj.weight`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18491f9992144bb09a196d903acd6b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbd067ca98b475b867b095217fa2eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea44c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_think> and <end_think>. \\nThen, provide your solution between <SOLUTION> and </SOLUTION>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Applying GRPO CHAT template \n",
    "\n",
    "## How important is the system prompt to guide the reasoning format for the model \n",
    "\n",
    "\n",
    "reasoning_start=\"<start_think>\"\n",
    "reasoning_end=\"<end_think>\"\n",
    "\n",
    "soln_start=\"<SOLUTION>\"\n",
    "soln_end=\"</SOLUTION>\" \n",
    "\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and {reasoning_end}. \n",
    "Then, provide your solution between {soln_start} and {soln_end}\"\"\"\n",
    "    \n",
    "system_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57422b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \\\n",
    "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "        \"{{ messages[0]['content'] + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages[1:] %}\"\\\n",
    "    \"{% else %}\"\\\n",
    "        \"{{ '{system_prompt}' + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages %}\"\\\n",
    "    \"{% endif %}\"\\\n",
    "    \"{% for message in loop_messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"{{ message['content'] }}\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"{{ message['content'] + eos_token }}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}{{ '{reasoning_start}' }}\"\\\n",
    "    \"{% endif %}\"\n",
    "\n",
    "chat_template = chat_template\\\n",
    "    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n",
    "    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5d9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_think> and <end_think>. \\nThen, provide your solution between <SOLUTION> and </SOLUTION><|im_end|>What is 1+1?<start_think>I think it's 2.<end_think><SOLUTION>2</SOLUTION><|im_end|>What is 2+2?<start_think>\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"What is 1+1?\"},\n",
    "    {\"role\" : \"assistant\", \"content\" : f\"{reasoning_start}I think it's 2.{reasoning_end}{soln_start}2{soln_end}\"},\n",
    "    {\"role\" : \"user\", \"content\" : \"What is 2+2?\"},\n",
    "], tokenize = False, add_generation_prompt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397eaab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-25632f7f-9790-4c6f-9c97-dddd9d2326a2\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>problem</th>\n",
       "      <th>generated_solution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>Given $\\sqrt{x^2+165}-\\sqrt{x^2-52}=7$ and $x$...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's see. I need to solve the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-2</td>\n",
       "      <td>Find the value of the parameter $a$ for which ...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to find the value of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>What is the sum of all real numbers $x$ for wh...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to solve the equation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>Evaluate the sum \\(\\sum_{n=1}^\\infty \\frac{\\ph...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to evaluate the infin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>30</td>\n",
       "      <td>What is the largest positive integer that divi...</td>\n",
       "      <td>&lt;think&gt;\\nAlright, so I need to find the larges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19243</th>\n",
       "      <td>244</td>\n",
       "      <td>Let \\( p \\), \\( q \\), and \\( r \\) be the disti...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to find the value of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19245</th>\n",
       "      <td>1</td>\n",
       "      <td>A bug is on the $0$ of a number line. At any p...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I have this problem where a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19247</th>\n",
       "      <td>4</td>\n",
       "      <td>A bus left point X for point Y. Two hours late...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem step ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19248</th>\n",
       "      <td>18</td>\n",
       "      <td>Each interior angle of a regular n-gon measure...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's see. I need to find the n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19250</th>\n",
       "      <td>0.8960</td>\n",
       "      <td>Find the probability that the second blue resu...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to find the probabili...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7507 rows × 3 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25632f7f-9790-4c6f-9c97-dddd9d2326a2')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-25632f7f-9790-4c6f-9c97-dddd9d2326a2 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-25632f7f-9790-4c6f-9c97-dddd9d2326a2');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      expected_answer                                            problem  \\\n",
       "0                  14  Given $\\sqrt{x^2+165}-\\sqrt{x^2-52}=7$ and $x$...   \n",
       "6                  -2  Find the value of the parameter $a$ for which ...   \n",
       "9                  18  What is the sum of all real numbers $x$ for wh...   \n",
       "13                  2  Evaluate the sum \\(\\sum_{n=1}^\\infty \\frac{\\ph...   \n",
       "17                 30  What is the largest positive integer that divi...   \n",
       "...               ...                                                ...   \n",
       "19243             244  Let \\( p \\), \\( q \\), and \\( r \\) be the disti...   \n",
       "19245               1  A bug is on the $0$ of a number line. At any p...   \n",
       "19247               4  A bus left point X for point Y. Two hours late...   \n",
       "19248              18  Each interior angle of a regular n-gon measure...   \n",
       "19250          0.8960  Find the probability that the second blue resu...   \n",
       "\n",
       "                                      generated_solution  \n",
       "0      <think>\\nOkay, let's see. I need to solve the ...  \n",
       "6      <think>\\nOkay, so I need to find the value of ...  \n",
       "9      <think>\\nOkay, so I need to solve the equation...  \n",
       "13     <think>\\nOkay, so I need to evaluate the infin...  \n",
       "17     <think>\\nAlright, so I need to find the larges...  \n",
       "...                                                  ...  \n",
       "19243  <think>\\nOkay, so I need to find the value of ...  \n",
       "19245  <think>\\nOkay, so I have this problem where a ...  \n",
       "19247  <think>\\nOkay, let's tackle this problem step ...  \n",
       "19248  <think>\\nOkay, let's see. I need to find the n...  \n",
       "19250  <think>\\nOkay, so I need to find the probabili...  \n",
       "\n",
       "[7507 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prefinetune for formatting \n",
    "# doesnt matter if i use openmathreasoning DATASET  \n",
    "# this is just to verify if the GRPO format is even working \n",
    "\n",
    "\n",
    "from datasets import load_dataset \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split = \"cot\")\n",
    "dataset = dataset.to_pandas()[\n",
    "    [\"expected_answer\", \"problem\", \"generated_solution\"]\n",
    "]\n",
    "\n",
    "# not sure why the expected number should only be a number why has unsloth filtered for this ? \n",
    "#\n",
    "is_number = pd.to_numeric(pd.Series(dataset[\"expected_answer\"]), errors = \"coerce\").notnull()\n",
    "# Select only numbers\n",
    "dataset = dataset.iloc[np.where(is_number)[0]]\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c33bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting dataset to follow the custom GRPO formatting \n",
    "\n",
    "def format_dataset(x):\n",
    "    exp_ans = x[\"expected_answer\"] \n",
    "    problem = x[\"problem\"] \n",
    "    \n",
    "    \n",
    "    thoughts = x[\"generated_solution\"]\n",
    "    thoughts = thoughts.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
    "    \n",
    "    thoughts = thoughts.strip() \n",
    "    \n",
    "    final_prompt = \\ \n",
    "        reasoning_start + thoughts + reasoning_end + \\ \n",
    "        soln_start + exp_ans + soln_end\n",
    "    return [\n",
    "        {\"role\" : \"system\", \"content\" : system_prompt}, \n",
    "        {\"role\":\"user\", \"content\": problem},\n",
    "        {\"role\": \"assistant\",  \"content\": final_prompt},\n",
    "    ]\n",
    "    \n",
    "dataset[\"Messages\"] = dataset.apply(format_dataset, axis=1) \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6251e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "    dataset[\"Messages\"][0], tokenizer = False \n",
    ")\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
