{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72baa110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94f7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "ds = load_dataset(\"IUTVanguard/PhysicsEval\", split=\"test\")\n",
    "\n",
    "def filter_category(row):\n",
    "    return row[\"category\"] in [\n",
    "        \"Classical Mechanics and Dynamics\",\n",
    "        \"Fluid Mechanics and Continuum Dynamics\"\n",
    "    ]\n",
    "\n",
    "filtered_ds = ds.filter(filter_category)\n",
    "\n",
    "print(f\"Filtered dataset size: {len(filtered_ds)}\")\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"ojus1/Qwen3-0.6B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "OUTPUT_FILE = \"qwen_physics_outputs.jsonl\"\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    for row in tqdm(filtered_ds):\n",
    "\n",
    "        prompt = row[\"simplified_problem_statement\"]\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "          messages,\n",
    "          add_generation_prompt=True,\n",
    "          tokenize=True,\n",
    "          return_dict=True,\n",
    "          return_tensors=\"pt\",\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=5000\n",
    "            )\n",
    "\n",
    "        gen_text = tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "\n",
    "        record = {\n",
    "            \"problem_id\": row[\"Problem_ID\"],\n",
    "            \"category\": row[\"category\"],\n",
    "            \"question\": row[\"simplified_problem_statement\"],\n",
    "            \"model_output\": gen_text,\n",
    "            \"reference_answer\": row[\"final_answers_in_brief\"],\n",
    "        }\n",
    "\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26530a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting jsonl2json\n",
      "  Using cached jsonl2json-1.0.0-py3-none-any.whl (3.9 kB)\n",
      "Installing collected packages: jsonl2json\n",
      "Successfully installed jsonl2json-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\dhruva rao\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\dhruva rao\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\dhruva rao\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\dhruva rao\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\dhruva rao\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\dhruva rao\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\dhruva rao\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.3\n",
      "[notice] To update, run: C:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install jsonl2json\n",
    "\n",
    "from jsonl2json import JsonlToJsonFormatter\n",
    "\n",
    "jsonl = JsonlToJsonFormatter('qwen3-0.6B_eval.jsonl', 'qwen3-0.6B_eval.json')\n",
    "jsonl.to_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9e654f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-1.2.3-py3-none-any.whl (520 kB)\n",
      "     -------------------------------------- 521.0/521.0 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting packaging>=20.9\n",
      "  Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached pyyaml-6.0.3-cp310-cp310-win_amd64.whl (158 kB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.20.2-py3-none-any.whl (16 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "     ---------------------------------------- 2.9/2.9 MB 2.0 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Collecting shellingham\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting tqdm>=4.42.1\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting typer-slim\n",
      "  Downloading typer_slim-0.21.0-py3-none-any.whl (47 kB)\n",
      "     ---------------------------------------- 47.2/47.2 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Collecting certifi\n",
      "  Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "     -------------------------------------- 152.9/152.9 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Collecting idna\n",
      "  Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Collecting anyio\n",
      "  Downloading anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "     -------------------------------------- 113.4/113.4 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting h11>=0.16\n",
      "  Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting click>=8.0.0\n",
      "  Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "     -------------------------------------- 108.3/108.3 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting exceptiongroup>=1.0.2\n",
      "  Using cached exceptiongroup-1.3.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: typing-extensions, shellingham, pyyaml, packaging, idna, hf-xet, h11, fsspec, filelock, colorama, certifi, tqdm, httpcore, exceptiongroup, click, typer-slim, anyio, httpx, huggingface_hub\n",
      "Successfully installed anyio-4.12.0 certifi-2026.1.4 click-8.3.1 colorama-0.4.6 exceptiongroup-1.3.1 filelock-3.20.2 fsspec-2025.12.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface_hub-1.2.3 idna-3.11 packaging-25.0 pyyaml-6.0.3 shellingham-1.5.4 tqdm-4.67.1 typer-slim-0.21.0 typing-extensions-4.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc82317d67fa46e5a395371fbc5f1781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install huggingface_hub \n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10135590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807e2395512144f1abfde1f30a8eeff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae4ea4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/DrDrunkenstein22/qwen_eval_ans/commit/3c357378f0eb83a6d5701071cd74d86be657ba93', commit_message='Upload qwen-0.6B_ans_physics_eval with huggingface_hub', commit_description='', oid='3c357378f0eb83a6d5701071cd74d86be657ba93', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/DrDrunkenstein22/qwen_eval_ans', endpoint='https://huggingface.co', repo_type='dataset', repo_id='DrDrunkenstein22/qwen_eval_ans'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=r\"D:\\physics-reasoning\\smol-phy-reasoning\\qwen3-0.6B_eval.jsonl\",\n",
    "    path_in_repo=\"qwen-0.6B_ans_physics_eval\",\n",
    "    repo_id=\"DrDrunkenstein22/qwen_eval_ans\",\n",
    "    repo_type=\"dataset\",\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4db5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen revised eval answers set w/ elaborate solution steps for comparison \n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "ds = load_dataset(\"IUTVanguard/PhysicsEval\", split=\"test\")\n",
    "\n",
    "def filter_category(row):\n",
    "    return row[\"category\"] in [\n",
    "        \"Classical Mechanics and Dynamics\",\n",
    "        \"Fluid Mechanics and Continuum Dynamics\"\n",
    "    ]\n",
    "\n",
    "filtered_ds = ds.filter(filter_category)\n",
    "print(f\"Filtered dataset size: {len(filtered_ds)}\")\n",
    "\n",
    "model_name = \"ojus1/Qwen3-0.6B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"balanced\"   # on kaggle 2*t4 gpus faster than auto device mapping \n",
    ")\n",
    "model.eval()\n",
    "\n",
    "OUTPUT_FILE = \"/kaggle/working/qwen_final_eval.json\"\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    for row in tqdm(filtered_ds):\n",
    "        prompt = row[\"simplified_problem_statement\"]\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "          messages,\n",
    "          add_generation_prompt=True,\n",
    "          tokenize=True,\n",
    "          return_dict=True,\n",
    "          return_tensors=\"pt\",\n",
    "        ).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=2048, \n",
    "                do_sample=False, \n",
    "                temperature=1.0\n",
    "            )\n",
    "        \n",
    "        gen_text = tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        \n",
    "        record = {\n",
    "            \"problem_id\": row[\"Problem_ID\"],\n",
    "            \"category\": row[\"category\"],\n",
    "            \"question\": row[\"simplified_problem_statement\"],\n",
    "            \"model_output\": gen_text,\n",
    "            \"elaborate_solution\": row[\"elaborated_solution_steps\"], \n",
    "            \"reference_answer\": row[\"final_answers_in_brief\"],\n",
    "        }\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce09ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\physics-reasoning\\\\smol-phy-reasoning\\\\qwen_final_phy_eval.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2156116317.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"D:\\physics-reasoning\\smol-phy-reasoning\\qwen_final_phy_eval_results_deepseek_chimera.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_jsonl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEvaluation Summary:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total items evaluated: {len(results)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2156116317.py\u001b[0m in \u001b[0;36mprocess_jsonl\u001b[0;34m(in_path, out_path)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mids_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\physics-reasoning\\\\smol-phy-reasoning\\\\qwen_final_phy_eval.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Pineapple_api_key = os.getenv(\"ANANNAS_API_KEY\")\n",
    "Openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def safe_json_loads(text):\n",
    "    if not text or not text.strip():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Try to extract first JSON object\n",
    "        start = text.find(\"{\")\n",
    "        end = text.rfind(\"}\")\n",
    "        if start != -1 and end != -1:\n",
    "            try:\n",
    "                return json.loads(text[start:end+1])\n",
    "            except json.JSONDecodeError:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_eval_prompt(problem_id, elaborate_solution, model_output, question):\n",
    "    prompt = f\"\"\"You are an expert physics problem evaluator. Your task is to meticulously and STRICTLY evaluate an AI-generated solution based on its own merits and against the provided elaborated solution steps.\n",
    "\n",
    "Evaluate the AI-generated solution based on the following categories and scoring guidelines. Provide your evaluation STRICTLY as a JSON object.\n",
    "\n",
    "Evaluation Categories and Scoring Guidelines:\n",
    "\n",
    "1. **mathematical_accuracy**: (Score 1-5)\n",
    "   How correct are the AI's calculations, numerical answers, and units?\n",
    "   * 5: All calculations, numerical results, and units are perfectly correct\n",
    "   * 4: Minor calculation error, but underlying method is sound\n",
    "   * 3: Several minor errors, or one significant calculation error\n",
    "   * 2: Major calculation errors or fundamental misunderstandings\n",
    "   * 1: Almost all calculations are incorrect or missing\n",
    "\n",
    "2. **logical_consistency**: (Score 1-5)\n",
    "   Does the AI solution follow a logical step-by-step progression?\n",
    "   * 5: Perfect logical flow, impeccable reasoning\n",
    "   * 4: Mostly logical, minor unclear step\n",
    "   * 3: Some logical gaps or inconsistencies\n",
    "   * 2: Significant logical flaws\n",
    "   * 1: Illogical or incoherent\n",
    "\n",
    "3. **completeness**: (Score 1-5)\n",
    "   Does the AI-generated solution address all parts of the problem?\n",
    "   * 5: All parts fully addressed\n",
    "   * 4: Minor aspect overlooked\n",
    "   * 3: Significant part ignored\n",
    "   * 2: Only small portion addressed\n",
    "   * 1: Largely unaddressed\n",
    "\n",
    "4. **clarity_and_coherence**: (Score 1-5)\n",
    "   Is the AI's explanation clear and easy to understand?\n",
    "   * 5: Exceptionally clear and well-structured\n",
    "   * 4: Clear with minor areas for improvement\n",
    "   * 3: Generally understandable but verbose or unclear in parts\n",
    "   * 2: Difficult to understand\n",
    "   * 1: Incomprehensible\n",
    "\n",
    "5. **formulas_principles**: (Score 1-5)\n",
    "   Are correct physical formulas and principles applied correctly?\n",
    "   * 5: All necessary formulas correctly identified and applied\n",
    "   * 4: Mostly correct with minor errors\n",
    "   * 3: Some incorrect formulas or significant misapplication\n",
    "   * 2: Major errors in formula/principle selection\n",
    "   * 1: Completely inappropriate formulas\n",
    "\n",
    "6. **assumptions_made**: (Score 1-5)\n",
    "   Are AI assumptions explicit, justified, and reasonable?\n",
    "   * 5: All assumptions explicitly stated and well-justified\n",
    "   * 4: Most assumptions stated and reasonable\n",
    "   * 3: Some key assumptions missing or questionable\n",
    "   * 2: Major unreasonable assumptions\n",
    "   * 1: Inappropriate or absent assumptions\n",
    "\n",
    "7. **overall_correctness**: (Score 0-10)\n",
    "   How correct is the AI's approach and final answer overall?\n",
    "   * 10: Perfect solution\n",
    "   * 8-9: Excellent, very minor flaws\n",
    "   * 6-7: Good, largely correct\n",
    "   * 4-5: Partially correct\n",
    "   * 2-3: Mostly incorrect\n",
    "   * 0-1: Completely incorrect\n",
    "\n",
    "Physics Question: {question}\n",
    "\n",
    "Problem ID: {problem_id}\n",
    "\n",
    "Elaborated Solution Steps: {elaborate_solution}\n",
    "\n",
    "AI-Generated Solution to Evaluate: {model_output}\n",
    "\n",
    "Provide ONLY a JSON object with the problem_id and scores for each category.\n",
    "\n",
    "Example JSON format:\n",
    "{{\n",
    "  \"problem_id\": \"{problem_id}\",\n",
    "  \"mathematical_accuracy\": <score_1_to_5>,\n",
    "  \"logical_consistency\": <score_1_to_5>,\n",
    "  \"completeness\": <score_1_to_5>,\n",
    "  \"clarity_and_coherence\": <score_1_to_5>,\n",
    "  \"formulas_principles\": <score_1_to_5>,\n",
    "  \"assumptions_made\": <score_1_to_5>,\n",
    "  \"overall_correctness\": <score_0_to_10>\n",
    "}}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def process_jsonl(in_path: Path, out_path: Path = None):\n",
    "    \"\"\"\n",
    "    Process JSONL file and evaluate each entry using LLM judge\n",
    "    \"\"\"\n",
    "    if out_path is None:\n",
    "        out_path = in_path.parent / f\"{in_path.stem}_evaluated.json\"\n",
    "    \n",
    "    qwen_eval_data = []\n",
    "    prob_ids_processed = set()\n",
    "    \n",
    "    # Load previously evaluated items if output file exists\n",
    "    if os.path.exists(out_path):\n",
    "        with open(out_path, 'r', encoding='utf-8') as f:\n",
    "            judge_data = json.load(f)\n",
    "            if isinstance(judge_data, list):\n",
    "                qwen_eval_data.extend(i for i in judge_data if isinstance(i, dict))\n",
    "                prob_ids_processed.update(i.get('problem_id') for i in qwen_eval_data if i.get('problem_id'))\n",
    "        logger.info(f\"Loaded {len(qwen_eval_data)} previously evaluated items from {out_path}\")\n",
    "    \n",
    "    ids_left = []\n",
    "    with open(in_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            problem_id = item.get('problem_id')\n",
    "            if problem_id not in prob_ids_processed:\n",
    "                if item.get('model_output') and item.get('elaborate_solution'):\n",
    "                    ids_left.append(item)\n",
    "    \n",
    "    logger.info(f\"Found {len(ids_left)} items to evaluate\")\n",
    "    \n",
    "    if not ids_left:\n",
    "        logger.info(\"No new items to evaluate\")\n",
    "        return qwen_eval_data\n",
    "    \n",
    "    client = OpenAI(\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=Openrouter_api_key\n",
    "    )\n",
    "    \n",
    "    for j, item in enumerate(ids_left):\n",
    "        problem_id = item.get('problem_id')\n",
    "        logger.info(f\"Processing item {j + 1}/{len(ids_left)}: {problem_id}\")\n",
    "        \n",
    "        prompt = fetch_eval_prompt(\n",
    "            problem_id,\n",
    "            item.get('elaborate_solution'),\n",
    "            item.get('model_output'),\n",
    "            item.get('question', '')\n",
    "        )\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"tngtech/deepseek-r1t-chimera:free\", \n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"physics_evaluation\",\n",
    "                    \"strict\": True,\n",
    "                    \"schema\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"problem_id\": {\"type\": \"string\"},\n",
    "                            \"mathematical_accuracy\": {\"type\": \"integer\"},\n",
    "                            \"logical_consistency\": {\"type\": \"integer\"},\n",
    "                            \"completeness\": {\"type\": \"integer\"},\n",
    "                            \"clarity_and_coherence\": {\"type\": \"integer\"},\n",
    "                            \"formulas_principles\": {\"type\": \"integer\"},\n",
    "                            \"assumptions_made\": {\"type\": \"integer\"},\n",
    "                            \"overall_correctness\": {\"type\": \"integer\"}\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"problem_id\", \"mathematical_accuracy\", \"logical_consistency\",\n",
    "                            \"completeness\", \"clarity_and_coherence\", \"formulas_principles\",\n",
    "                            \"assumptions_made\", \"overall_correctness\"\n",
    "                        ],\n",
    "                        \"additionalProperties\": False\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            temperature=0.2,\n",
    "            # max_tokens=1024,\n",
    "            # top_p=0.5\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        logger.info(f\"RAW MODEL OUTPUT:\\n{repr(content)}\")\n",
    "\n",
    "        \n",
    "        eval_result = safe_json_loads(content)\n",
    "        if eval_result is None:\n",
    "            logger.error(f\"Invalid JSON for problem_id={problem_id}\")\n",
    "            continue\n",
    "\n",
    "        qwen_eval_data.append(eval_result)\n",
    "        \n",
    "        # checkpointing evals \n",
    "        with open(out_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(qwen_eval_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logger.info(f\"Evaluated {problem_id}: Overall correctness = {eval_result.get('overall_correctness')}/10\")\n",
    "    \n",
    "    logger.info(f\"Evaluation complete. Total evaluated: {len(qwen_eval_data)}\")\n",
    "    logger.info(f\"Results saved to: {out_path}\")\n",
    "    \n",
    "    return qwen_eval_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = Path(r\"D:\\physics-reasoning\\smol-phy-reasoning\\qwen_final_phy_eval.json\")\n",
    "    output_file = Path(r\"D:\\physics-reasoning\\smol-phy-reasoning\\qwen_final_phy_eval_results_deepseek_chimera.json\")\n",
    "    \n",
    "    results = process_jsonl(input_file, output_file)\n",
    "    print(f\"\\nEvaluation Summary:\")\n",
    "    print(f\"Total items evaluated: {len(results)}\")\n",
    "    if results:\n",
    "        avg_score = sum(r.get('overall_correctness', 0) for r in results) / len(results)\n",
    "        print(f\"Average overall correctness: {avg_score:.2f}/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a129e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
